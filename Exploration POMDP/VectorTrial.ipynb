{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent, environments\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import t, mode\n",
    "from tqdm import tqdm\n",
    "import cProfile\n",
    "\n",
    "from utility import print_gridworld_with_policy, print_heatmap, plot_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='GridworldPOMDPEnvGoalless-v0',\n",
    "    entry_point=environments.GridworldPOMDPEnvGoalless\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld Vector First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes for the training\n",
    "n_episodes = 2000\n",
    "n_traj = 100\n",
    "# Number of runs per episode\n",
    "n_run = 4\n",
    "# Define the arguments for each environment\n",
    "time_horizon = 25\n",
    "steepness = 15\n",
    "prob = 0\n",
    "envs = gym.vector.make('GridworldPOMDPEnvGoalless-v0', time_horizon=time_horizon, steepness=steepness, prob=prob, num_envs=n_traj)\n",
    "env = environments.GridworldPOMDPEnvGoalless(time_horizon = time_horizon, steepness=15, prob=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = env.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 12/8000 [00:08<1:30:09,  1.48it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duilio999/.pyenv/versions/3.10.12/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/duilio999/.pyenv/versions/3.10.12/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/duilio999/.pyenv/versions/3.10.12/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m done \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_traj, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(done):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# Sample action and get probabilities from the belief\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     actions, probs \u001b[39m=\u001b[39m ag\u001b[39m.\u001b[39;49mget_actions()\n\u001b[1;32m     21\u001b[0m     \u001b[39m# Sample state\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     sampled_states \u001b[39m=\u001b[39m ag\u001b[39m.\u001b[39mget_states()\n",
      "File \u001b[0;32m/media/duilio999/0432A42F32A4281E/Users/dulio/PycharmProjects/Thesis/Exploration POMDP/agent.py:487\u001b[0m, in \u001b[0;36mREINFORCEAgentEPOMDPVec.get_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mThis method is used to sample the actions of each parallel environment given the current beliefs.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    486\u001b[0m probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_probability(state) \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeliefs])\n\u001b[0;32m--> 487\u001b[0m actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlen\u001b[39m(prob), p\u001b[39m=\u001b[39mprob) \u001b[39mfor\u001b[39;00m prob \u001b[39min\u001b[39;00m probs])\n\u001b[1;32m    488\u001b[0m \u001b[39mreturn\u001b[39;00m actions, probs\n",
      "File \u001b[0;32m/media/duilio999/0432A42F32A4281E/Users/dulio/PycharmProjects/Thesis/Exploration POMDP/agent.py:487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mThis method is used to sample the actions of each parallel environment given the current beliefs.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    486\u001b[0m probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_probability(state) \u001b[39mfor\u001b[39;00m state \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeliefs])\n\u001b[0;32m--> 487\u001b[0m actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mlen\u001b[39;49m(prob), p\u001b[39m=\u001b[39;49mprob) \u001b[39mfor\u001b[39;00m prob \u001b[39min\u001b[39;00m probs])\n\u001b[1;32m    488\u001b[0m \u001b[39mreturn\u001b[39;00m actions, probs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=n_run * n_episodes, ncols=80) as pbar:\n",
    "    # Train the agent and plot the entropies\n",
    "    list_entropies = []\n",
    "    list_true_entropies = []\n",
    "    for r in range(n_run):\n",
    "        ag = agent.REINFORCEAgentEPOMDPVec(env, alpha=0.5, n_traj=n_traj)  # Modify the agent to accept the parallel environments\n",
    "        avg_entropies = []\n",
    "        avg_true_entropies = []\n",
    "        for i in range(n_episodes):\n",
    "            trajectories = []\n",
    "            episodes = [ [] for _ in range(n_traj) ]\n",
    "            true_entropies = []\n",
    "            d_t = np.zeros((n_traj, env.observation_space.n))  # Initialize visitation counts for all parallel environments\n",
    "            true_d_t = np.zeros((n_traj, env.observation_space.n))\n",
    "            envs.reset()\n",
    "            ag.beliefs = np.ones((n_traj, env.observation_space.n)) / env.observation_space.n\n",
    "            done = np.zeros(n_traj, dtype=bool)\n",
    "            while not np.all(done):\n",
    "                # Sample action and get probabilities from the belief\n",
    "                actions, probs = ag.get_actions()\n",
    "                # Sample state\n",
    "                sampled_states = ag.get_states()\n",
    "                # Take a step in the parallel environments\n",
    "                next_obs, rewards, done, _, true_states = envs.step(actions)\n",
    "                # Get the indices of the states for all parallel environments\n",
    "                state_indices = [env.state_to_index(state) for state in sampled_states]\n",
    "                true_state_indices = true_states['true_state']\n",
    "                # Update state visitation counts for all parallel environments\n",
    "                for i, state_index in enumerate(state_indices):\n",
    "                    d_t[i][state_index] += 1\n",
    "                for i, true_state_index in enumerate(true_state_indices):\n",
    "                    true_d_t[i][true_state_index] += 1\n",
    "                # Arrange the single trajectories\n",
    "                for i in range(n_traj):\n",
    "                    episodes[i].append((ag.beliefs[i], actions[i], probs[i], rewards[i], true_state_indices[i]))\n",
    "                # Update belief\n",
    "                ag.belief_update(actions, next_obs)\n",
    "            # Compute true entropy of the trajectory for all parallel environments\n",
    "            true_d_t /= time_horizon\n",
    "            log_true_d_t = -np.log(true_d_t, where=true_d_t>0)\n",
    "            true_entropies = np.sum(np.multiply(true_d_t, log_true_d_t),axis=1)\n",
    "            # Compute believed entropy of the trajectory for all parallel environments\n",
    "            d_t /= time_horizon\n",
    "            log_d_t = -np.log(d_t, where=d_t>0)\n",
    "            entropies = np.sum(np.multiply(d_t, log_d_t), axis=1)\n",
    "            for i in range(n_traj):\n",
    "                trajectories.append((episodes[i], entropies[i]))\n",
    "            ag.update_multiple_sampling(trajectories)\n",
    "            avg_entropies.append(np.mean(entropies))\n",
    "            avg_true_entropies.append(np.mean(true_entropies))\n",
    "            pbar.update(1)\n",
    "        ag.print_visuals(envs=envs, env=env, n_traj=n_traj)  # Modify the agent to accept the parallel environments\n",
    "        list_entropies.append(avg_entropies)\n",
    "        list_true_entropies.append(avg_true_entropies)\n",
    "    list_entropies = np.transpose(np.array(list_entropies), (1, 0))\n",
    "    list_true_entropies = np.transpose(np.array(list_true_entropies), (1, 0))\n",
    "\n",
    "plot_graph(n_run, n_episodes, list_entropies, list_true_entropies, confidence)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1764de405fde2f6b6e670feabbb227954f20948d4d8b80d8e6259c45155a2c52"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
