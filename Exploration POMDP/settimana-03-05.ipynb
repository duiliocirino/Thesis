{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:22:10.919429Z",
     "iopub.execute_input": "2023-05-08T13:22:10.919876Z",
     "iopub.status.idle": "2023-05-08T13:22:11.249085Z",
     "shell.execute_reply.started": "2023-05-08T13:22:10.919844Z",
     "shell.execute_reply": "2023-05-08T13:22:11.248040Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gridworld MDP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class GridworldEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a custom made Gym environment of a simple Gridworld, a NxN matrix where the agent starts from a starting position I and \n",
    "    has to reach the goal position G.\n",
    "    \n",
    "    The set of states S = {[x,y]| x,y ∈ [0, ..., grid_size]} or {}, represent the possible positions in the environment.\n",
    "    The set of actions A = [up, down, left, right].\n",
    "    The transition to a new state T(s_t+1 | s_t, a_t).\n",
    "    The reward R(s, a) = {1 if s == G else -0.1}.\n",
    "    \n",
    "    In this version the initial state is the position 0 ([0, 0]), I = [0, 0]. The goal state is the position 24 ([4, 4]) G = [4,4]. \n",
    "    \n",
    "    Args:\n",
    "    - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
    "    - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
    "    - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_space = spaces.Discrete(self.grid_size ** 2)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.reward_range = (-0.1, 1.0)\n",
    "        self.goal = (grid_size-1, grid_size-1)\n",
    "        self.current_pos = (0, 0)\n",
    "        self.done = False\n",
    "        self.time_horizon = time_horizon\n",
    "        self.steps_taken = 0\n",
    "        self.prob = prob\n",
    "        self.transition_matrix = self._build_transition_matrix()\n",
    "        \n",
    "    def _build_transition_matrix(self):\n",
    "        '''\n",
    "        This method builds the transition matrix for the MDP, T(s'|a, s).\n",
    "        The function should be used with the following order of operands:\n",
    "         - first parameter: s, the state where the agent takes the action in\n",
    "         - second parameter: s', the state where the agent arrives taking action a from state s\n",
    "         - third parameter: a, the action taken by the agent\n",
    "        '''\n",
    "        transition_matrix = np.zeros((self.grid_size**2, self.grid_size**2, self.action_space.n))\n",
    "        # For every state (i,j)\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                s = (i, j)\n",
    "                # For every action 'a' chosen as action from the agent\n",
    "                for a in range(self.action_space.n):\n",
    "                    # For all actions 'a_'\n",
    "                    for a_ in range(self.action_space.n):\n",
    "                        # Calculate the probability of \n",
    "                        prob = 1 - self.prob if a_ == a else self.prob / (self.action_space.n - 1)\n",
    "                        s_ = self._sample_new_position(a_, s)\n",
    "                        transition_matrix[self._state_to_index(s), self._state_to_index(s_), a] += prob\n",
    "        return transition_matrix\n",
    "    \n",
    "    def _sample_new_position(self, action, state):\n",
    "        if action == 0:  # up\n",
    "            new_pos = (state[0], max(0, state[1]-1))\n",
    "        elif action == 1:  # down\n",
    "            new_pos = (state[0], min(self.grid_size-1, state[1]+1))\n",
    "        elif action == 2:  # left\n",
    "            new_pos = (max(0, state[0]-1), state[1])\n",
    "        elif action == 3:  # right\n",
    "            new_pos = (min(self.grid_size-1, state[0]+1), state[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        return new_pos\n",
    "    \n",
    "    def _state_to_index(self, state):\n",
    "        return state[0] + state[1] * self.grid_size\n",
    "\n",
    "    def _index_to_state(self, index):\n",
    "        \"\"\"Converts an index to a state tuple (i, j).\"\"\"\n",
    "        i = index % self.grid_size\n",
    "        j = index // self.grid_size\n",
    "        #print(\"Converting %d => (%d, %d)\" %(index, i, j))\n",
    "        return (i, j)\n",
    "    \n",
    "    def sample_next_state(self, action):\n",
    "        current_state = self._state_to_index(self.current_pos)\n",
    "        action_probabilities = self.transition_matrix[current_state, :, action]\n",
    "        next_state_index = np.random.choice(self.grid_size**2, p=action_probabilities)\n",
    "        next_pos = self._index_to_state(next_state_index)\n",
    "        #print(\"Taking action %d from cur_pos index:%d (%d, %d): going to index %d which is state => (%d, %d)\" %(action, current_state, self.current_pos[0], self.current_pos[1], next_state_index, next_pos[0], next_pos[1]))\n",
    "        return next_pos\n",
    "        \n",
    "    def step(self, action):\n",
    "        #print(\"Timestep: %d\" %self.steps_taken)\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has already ended.\")\n",
    "        new_pos = self.sample_next_state(action)\n",
    "        reward = -0.1  # default reward for moving\n",
    "        if new_pos == self.goal:\n",
    "            reward = 1.0\n",
    "            self.done = True\n",
    "        self.current_pos = new_pos\n",
    "        self.steps_taken += 1\n",
    "        if self.time_horizon != -1 and self.steps_taken >= self.time_horizon:\n",
    "            self.done = True\n",
    "        return self.current_pos, reward, self.done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_pos = (0, 0)\n",
    "        self.done = False\n",
    "        self.steps_taken = 0\n",
    "        return self.current_pos\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in range(self.grid_size):\n",
    "                for i in range(self.grid_size):\n",
    "                    if (i, j) == self.current_pos:\n",
    "                        print(\"X \", end='')\n",
    "                    elif (i, j) == self.goal:\n",
    "                        print(\"G \", end='')\n",
    "                    else:\n",
    "                        print(\"_ \", end='')\n",
    "                print()\n",
    "            print()\n",
    "            \n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                states.append((i, j))\n",
    "        return states"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:22:12.810768Z",
     "iopub.execute_input": "2023-05-08T13:22:12.811128Z",
     "iopub.status.idle": "2023-05-08T13:22:12.833939Z",
     "shell.execute_reply.started": "2023-05-08T13:22:12.811100Z",
     "shell.execute_reply": "2023-05-08T13:22:12.832737Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reward Reinforce Agent MDP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the REINFORCE agent\n",
    "class REINFORCEAgent():\n",
    "    '''\n",
    "    This class is the implementation of a REINFORCE agent that tries to maximize the objective function J(θ)=E_(τ ~ p_π)[R(τ)]\n",
    "    Args:\n",
    "     - env: the instance of the environment on which the agent is acting.\n",
    "     - alpha: the value of the learning rate to compute the policy update.\n",
    "     - gamma: the value of the discount for the reward in order to compute the discounted reward.\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.policy_params = np.zeros((env.grid_size, env.grid_size, env.action_space.n))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Sample action from the policy\n",
    "        params = self.policy_params[state[0], state[1], :]\n",
    "        probs = np.exp(params) / np.sum(np.exp(params))\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action\n",
    "\n",
    "    def compute_returns(self, episode):\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for t in reversed(range(len(episode))):\n",
    "            _, _, reward, _ = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            returns.append(G)\n",
    "        returns = np.array(list(reversed(returns)))\n",
    "        return returns\n",
    "\n",
    "    def update(self, episode):\n",
    "        '''\n",
    "        This version of the update is the Monte Carlo sampling version of the REINFORCE algorithm.\n",
    "        \n",
    "        Args:\n",
    "         - episode: the sampled trajectory from which we compute the policy gradient.\n",
    "        '''\n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(episode)\n",
    "        \n",
    "        # Compute the policy gradient\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, _, _ = episode[t]\n",
    "            params = self.policy_params[state[0], state[1], :]\n",
    "            probs = np.exp(params) / np.sum(np.exp(params))\n",
    "            dlogp = np.zeros_like(params)\n",
    "            for i in range(env.action_space.n):\n",
    "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "            grad[state[0], state[1], :] += dlogp * returns[t]\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        \n",
    "    def update_gradient_approx(self, trajectories):\n",
    "        '''\n",
    "        This version of the update takes into consideration the approximation of the gradient approach instead of the single sampling. Instead\n",
    "        of working with only one trajectory it works with multiple trajectories in order to have a more accurate representation of the expected\n",
    "        value of the ∇J(θ).\n",
    "        \n",
    "        Args:\n",
    "         - trajectories: a list of sampled trajectories from which we compute the policy gradient.\n",
    "         \n",
    "        '''\n",
    "        # Compute the policy gradient\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for episode in trajectories:\n",
    "            for t in range(len(episode)):\n",
    "                returns = self.compute_returns(episode)\n",
    "                state, action, _, _ = episode[t]\n",
    "                # Compute the policy gradient\n",
    "                params = self.policy_params[state[0], state[1], :]\n",
    "                probs = np.exp(params) / np.sum(np.exp(params))\n",
    "                dlogp = np.zeros_like(params)\n",
    "                for i in range(env.action_space.n):\n",
    "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "                grad[state[0], state[1], :] += dlogp * returns[t]\n",
    "        grad /= len(trajectories)\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:22:16.342767Z",
     "iopub.execute_input": "2023-05-08T13:22:16.343128Z",
     "iopub.status.idle": "2023-05-08T13:22:16.358867Z",
     "shell.execute_reply.started": "2023-05-08T13:22:16.343101Z",
     "shell.execute_reply": "2023-05-08T13:22:16.357636Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test single sampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the Gridworld environment and the REINFORCE agent\n",
    "trajectories = 1000\n",
    "env = GridworldEnv(time_horizon=200)\n",
    "agent = REINFORCEAgent(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_rewards = []\n",
    "avg_rewards = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    agent.update(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards[-trajectories:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "    ax.clear()\n",
    "    ax.plot(total_rewards, label='Total Reward')\n",
    "    ax.plot(avg_rewards, label='Average Reward (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-06T14:15:38.496346Z",
     "iopub.execute_input": "2023-05-06T14:15:38.497239Z",
     "iopub.status.idle": "2023-05-06T14:17:08.985397Z",
     "shell.execute_reply.started": "2023-05-06T14:15:38.497191Z",
     "shell.execute_reply": "2023-05-06T14:17:08.984397Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test gradient approximation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the Gridworld environment and the REINFORCE agent\n",
    "trajectories = 1000\n",
    "n_traj = 20\n",
    "env = GridworldEnv(time_horizon=100)\n",
    "agent = REINFORCEAgent(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_avg_rewards = []\n",
    "avg_avg_rewards = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    trajectories = []\n",
    "    total_rewards = []\n",
    "    for k in range(n_traj):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        trajectories.append(episode)\n",
    "    agent.update_gradient_approx(trajectories)\n",
    "    total_avg_rewards.append(np.mean(total_rewards[-n_traj:]))\n",
    "    avg_avg_reward = np.mean(total_avg_rewards[-trajectories:])\n",
    "    avg_avg_rewards.append(avg_avg_reward)\n",
    "    ax.clear()\n",
    "    ax.plot(total_avg_rewards, label='Total Reward')\n",
    "    ax.plot(avg_avg_rewards, label='Average Reward (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-06T14:17:08.986975Z",
     "iopub.execute_input": "2023-05-06T14:17:08.987292Z",
     "iopub.status.idle": "2023-05-06T14:19:00.369822Z",
     "shell.execute_reply.started": "2023-05-06T14:17:08.987267Z",
     "shell.execute_reply": "2023-05-06T14:19:00.368803Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entropy Reinforce Agent MDP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class REINFORCEAgentE(REINFORCEAgent):\n",
    "    '''\n",
    "    This class is the extension of the previous Reinforce agent. The only change is the objective followed by the agent that this time is going\n",
    "    to be the state visitation entropy J(θ) = E_(τ ~ p_π)[H(d_τ(s))] .\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.01, gamma=0.8):\n",
    "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
    "        # Optional initialization to have a non-uniform initial policy\n",
    "        self.policy_params = self.create_stupid_policy()\n",
    "        \n",
    "    def create_stupid_policy(self):\n",
    "        params = np.zeros((env.grid_size, env.grid_size, env.action_space.n))\n",
    "        for i in range(self.env.grid_size):\n",
    "            for j in range(self.env.grid_size):\n",
    "                params[i,j,0] = 1\n",
    "        return params\n",
    "    \n",
    "    def compute_state_distribution(self, episode, t):\n",
    "        d_t = np.zeros((env.grid_size, env.grid_size))\n",
    "        for t in range (t + 1):\n",
    "            _, _ , _, s_t = episode[t]\n",
    "            d_t[s_t[0]][s_t[1]] += 1\n",
    "        d_t /= (t + 1)\n",
    "        return d_t\n",
    "\n",
    "    def update(self, episode):\n",
    "        # Compute the t-step state distribution for the current policy\n",
    "        d_t = self.compute_state_distribution(episode, len(episode) - 1)\n",
    "        # Compute entropy of d_t\n",
    "        entropy = 0\n",
    "        for row in d_t:\n",
    "            for s in row:\n",
    "                if s != 0:\n",
    "                    entropy -= s * np.log(s)\n",
    "        \n",
    "        # Update policy parameters using the gradient of the entropy of the t-step state distribution\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, _, _ = episode[t]\n",
    "            # Compute the policy gradient\n",
    "            params = self.policy_params[state[0], state[1], :]\n",
    "            probs = np.exp(params) / np.sum(np.exp(params))\n",
    "            dlogp = np.zeros_like(params)\n",
    "            for i in range(env.action_space.n):\n",
    "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "            grad[state[0], state[1], :] += dlogp \n",
    "        grad *= entropy\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        return entropy, d_t\n",
    "    \n",
    "    def update_gradient_approx(self, trajectories):\n",
    "        # Update policy parameters using the approximated gradient of the entropy objective function\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        entropies = []\n",
    "        for episode in trajectories:\n",
    "            # Initialize the gradient of the k-th sampled trajectory\n",
    "            grad_k = np.zeros_like(self.policy_params)\n",
    "            # Compute the t-step state distribution for the current policy\n",
    "            d_t = self.compute_state_distribution(episode, len(episode) - 1)\n",
    "            # Compute entropy of d_t\n",
    "            entropy = 0\n",
    "            for row in d_t:\n",
    "                for s in row:\n",
    "                    if s != 0:\n",
    "                        entropy -= s * np.log(s)\n",
    "            for t in range(len(episode)):\n",
    "                state, action, _, _ = episode[t]\n",
    "                # Compute the policy gradient\n",
    "                params = self.policy_params[state[0], state[1], :]\n",
    "                probs = np.exp(params) / np.sum(np.exp(params))\n",
    "                dlogp = np.zeros_like(params)\n",
    "                for i in range(env.action_space.n):\n",
    "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "                grad_k[state[0], state[1], :] += dlogp \n",
    "            grad_k *= entropy\n",
    "            # Sum the k-th gradient to the final gradient\n",
    "            grad += grad_k\n",
    "            entropies.append(entropy)\n",
    "        # Divide the gradient by the number of trajectories sampled\n",
    "        grad /= len(trajectories)\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        return entropies"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:22:22.386836Z",
     "iopub.execute_input": "2023-05-08T13:22:22.387188Z",
     "iopub.status.idle": "2023-05-08T13:22:22.404243Z",
     "shell.execute_reply.started": "2023-05-08T13:22:22.387162Z",
     "shell.execute_reply": "2023-05-08T13:22:22.403115Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test single sampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "trajectories = 1000\n",
    "env = GridworldEnv(time_horizon = 200, prob = 0.1)\n",
    "agent = REINFORCEAgentE(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_rewards = []\n",
    "avg_rewards = []\n",
    "entropies = []\n",
    "avg_entropies = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    #print(\"start episode %d\" %i)\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    #print(\"F\")\n",
    "    state = env.reset()\n",
    "    entropy, d_t = agent.update(episode)\n",
    "    '''\n",
    "    if i % 100 == 0:\n",
    "        print(d_t)\n",
    "    '''\n",
    "    entropies.append(entropy)\n",
    "    total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards[-trajectories:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "    avg_entropy = np.mean(entropies[-trajectories:])\n",
    "    avg_entropies.append(avg_entropy)\n",
    "    \n",
    "    #Plot Reward and Entropy\n",
    "    ax.clear()\n",
    "    #ax.plot(total_rewards, label='Total Reward')\n",
    "    #ax.plot(avg_rewards, label='Average Reward (%d episodes)' %episodes)\n",
    "    ax.plot(entropies, label='Entropy')\n",
    "    ax.plot(avg_entropies, label='Average Entropy (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward/Entropy')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:22:25.687051Z",
     "iopub.execute_input": "2023-05-08T13:22:25.688231Z",
     "iopub.status.idle": "2023-05-08T13:24:15.953754Z",
     "shell.execute_reply.started": "2023-05-08T13:22:25.688169Z",
     "shell.execute_reply": "2023-05-08T13:24:15.952900Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test gradient approximation**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "episodes = 1000\n",
    "n_traj = 50\n",
    "env = GridworldEnv(time_horizon = 200, prob = 0)\n",
    "agent = REINFORCEAgentE(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "avg_entropies = []\n",
    "avg_avg_entropies = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(episodes):\n",
    "    trajectories = []\n",
    "    entropies = []\n",
    "    for k in range(n_traj):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        trajectories.append(episode)\n",
    "    entropies = agent.update_gradient_approx(trajectories)\n",
    "    avg_entropies.append(np.mean(entropies[-n_traj:]))\n",
    "    avg_avg_entropy = np.mean(avg_entropies[-episodes:])\n",
    "    avg_avg_entropies.append(avg_avg_entropy)\n",
    "\n",
    "    #Plot Reward and Entropy\n",
    "    ax.clear()\n",
    "    ax.plot(avg_entropies, label='Entropy')\n",
    "    ax.plot(avg_avg_entropies, label='Average Entropy (%d episodes)' %episodes)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward/Entropy')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-06T15:14:04.666820Z",
     "iopub.execute_input": "2023-05-06T15:14:04.667251Z",
     "iopub.status.idle": "2023-05-06T15:27:18.157204Z",
     "shell.execute_reply.started": "2023-05-06T15:14:04.667219Z",
     "shell.execute_reply": "2023-05-06T15:27:18.156114Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gridworld POMDP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class GridworldPOMDPEnv(GridworldEnv):\n",
    "    '''\n",
    "    This class implements the extension of the upper Gridworld MDP, making it become a POMDP by inserting the observation mechanism.\n",
    "    The observations here are modeled as a probability of returning the real state of the MDP, so the agent can see the real state with\n",
    "    probability obs_prob and the others with probability 1 - obs_prob.\n",
    "    \n",
    "    Args:\n",
    "     - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
    "     - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
    "     - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
    "     - obs_prob: the probability with which the environment gives back the actual state \n",
    "    '''\n",
    "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, obs_prob = 0.8):\n",
    "        # Initialize the underlying Gridworld MDP\n",
    "        super().__init__(grid_size=grid_size, time_horizon=time_horizon, prob=prob)\n",
    "        # Initialize all the POMDP specific variables\n",
    "        self.observation_space = spaces.Discrete(self.grid_size**2)\n",
    "        self.obs_prob = obs_prob\n",
    "        self.observation_matrix = self.build_observation_matrix()\n",
    "        \n",
    "    def build_observation_matrix(self):\n",
    "        '''\n",
    "        This method creates the observation matrix for our environment.\n",
    "        The observation function indexes are to be used in order:\n",
    "         - first param: true state s\n",
    "         - second param: observation o\n",
    "        '''\n",
    "        # Initialize the observation function with zeros\n",
    "        observation_matrix = np.zeros((self.grid_size**2, self.grid_size**2))\n",
    "        for s_i in range(self.observation_space.n):\n",
    "            for s in range(self.observation_space.n):\n",
    "                # For every state, put probabilty to obs_prob if the outer state is equal to the inner, otherwise to the uniform between the other states.\n",
    "                observation_matrix[s_i][s] += self.obs_prob if s == s_i else (1 - self.obs_prob) / (self.observation_space.n - 1)\n",
    "        # Return the built matrix\n",
    "        return observation_matrix\n",
    "\n",
    "    def reset(self):\n",
    "        # Call the upper reset of the environment\n",
    "        super().reset()\n",
    "        # Set the initial belief to a uniform distribution and give it to the agent\n",
    "        initial_belief = np.ones(self.observation_space.n) / self.observation_space.n\n",
    "        return initial_belief\n",
    "\n",
    "    def step(self, action):\n",
    "        # Make the step of the underlying MDP\n",
    "        true_state, reward, done, info = super().step(action)\n",
    "        # Get the index of the state\n",
    "        true_state_index = self._state_to_index(true_state)\n",
    "        # Get the observation probabilities for the state\n",
    "        obs_prob = self.observation_matrix[true_state_index]\n",
    "        # Sample the next observation from the probabilities\n",
    "        obs = np.random.choice(self.grid_size**2, p=obs_prob)\n",
    "        # Change index to position\n",
    "        obs = self._index_to_state(obs)\n",
    "        return obs, reward, done, info"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T13:35:46.805454Z",
     "iopub.execute_input": "2023-05-08T13:35:46.805859Z",
     "iopub.status.idle": "2023-05-08T13:35:46.819136Z",
     "shell.execute_reply.started": "2023-05-08T13:35:46.805828Z",
     "shell.execute_reply": "2023-05-08T13:35:46.818103Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reward Reinforce POMDP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class REINFORCEAgentPOMDP(REINFORCEAgent):\n",
    "    '''\n",
    "    This calls implements the Reinforce Agent for the POMDP defined above.\n",
    "    It is implemented as the extension of the former MDP agent but adds the belief state and what is concerned by it.\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99):\n",
    "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
    "        self.belief = np.ones(env.grid_size**2)/(env.grid_size * env.grid_size)\n",
    "        self.last_sampled_state = 0\n",
    "        self.n_expected_value = 50\n",
    "    \n",
    "    # TODO: testino funzionamento\n",
    "    def belief_update(self, action, observation):\n",
    "        '''\n",
    "        This method updates the belief of the agent in a Bayesian way. \n",
    "        '''\n",
    "        # Get the index of the observed state\n",
    "        obs_state_index = self.env._state_to_index(observation)\n",
    "        # Initialize the updated belief vector\n",
    "        unnormalized_updated_belief = np.zeros(self.belief.shape)\n",
    "        for s in range(env.state_space.n):\n",
    "            # Get O(o\\s)\n",
    "            obs_o_s = self.env.observation_matrix[s, obs_state_index]\n",
    "            # Calculate the sum over all states\n",
    "            summation = np.sum(self.env.transition_matrix[s, :, action] * self.belief, axis=0)\n",
    "            b_s = obs_o_s * summation\n",
    "            unnormalized_updated_belief[s] = b_s\n",
    "        # Normalize the belief and update it\n",
    "        self.belief = unnormalized_updated_belief / np.sum(unnormalized_updated_belief)\n",
    "        \n",
    "    def get_state(self, mode):\n",
    "        '''\n",
    "        This method is used to sample or compute the expected state given the current belief.\n",
    "        Args:\n",
    "         - mode: if 0 it does a single sampling of the state.\n",
    "                 if 1 it gives the approximated expected value.\n",
    "                 No other values allowed.\n",
    "        '''\n",
    "        if mode != 0 and mode != 1:\n",
    "            raise Exception(\"You have to pass me 0 or 1, read :/\")\n",
    "        \n",
    "        if mode == 0:\n",
    "            state = np.random.choice(self.belief.size, p=self.belief)\n",
    "            self.last_sampled_state = state\n",
    "            state = env._index_to_state(state)\n",
    "            return state\n",
    "        \n",
    "        if mode == 1:\n",
    "            states = []\n",
    "            for i in range(self.n_expected_value):\n",
    "                states.append(np.random.choice(self.belief.size, p=self.belief))\n",
    "            state = max(set(states), key = states.count)\n",
    "            self.last_sampled_state = state\n",
    "            state = env._index_to_state(state)\n",
    "            return state"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T14:13:01.942624Z",
     "iopub.execute_input": "2023-05-08T14:13:01.943633Z",
     "iopub.status.idle": "2023-05-08T14:13:01.956000Z",
     "shell.execute_reply.started": "2023-05-08T14:13:01.943558Z",
     "shell.execute_reply": "2023-05-08T14:13:01.954762Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Mini Test Belief Update**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "env = GridworldPOMDPEnv(time_horizon=100)\nagent = REINFORCEAgentPOMDP(env)\n\n# TODO",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Test code**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create the Gridworld environment and the REINFORCE agent\nepisodes = 500\nenv = GridworldPOMDPEnv(time_horizon=100)\nagent = REINFORCEAgentPOMDP(env)\n\n# Train the agent and plot the rewards\ntotal_rewards = []\navg_rewards = []\nfig, ax = plt.subplots()\nfor i in range(episodes):\n    episode = []\n    agent.belief = env.reset()\n    done = False\n    total_reward = 0\n    while not done:\n        sampled_state = agent.get_state(1)\n        action = agent.get_action(sampled_state)\n        next_obs, reward, done, _ = env.step(action)\n        episode.append((sampled_state, action, reward, next_obs))\n        agent.belief_update(action, next_obs)\n        total_reward += reward\n    agent.update(episode)\n    total_rewards.append(total_reward)\n    avg_reward = np.mean(total_rewards[-episodes:])\n    avg_rewards.append(avg_reward)\n    ax.clear()\n    ax.plot(total_rewards, label='Total Reward')\n    ax.plot(avg_rewards, label='Average Reward (%d episodes)' %episodes)\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Reward')\n    ax.legend()\n    fig.canvas.draw()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-08T14:13:04.835996Z",
     "iopub.execute_input": "2023-05-08T14:13:04.836641Z",
     "iopub.status.idle": "2023-05-08T14:14:00.789348Z",
     "shell.execute_reply.started": "2023-05-08T14:13:04.836604Z",
     "shell.execute_reply": "2023-05-08T14:14:00.788195Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Entropy Reinforce POMDP",
   "metadata": {}
  }
 ]
}