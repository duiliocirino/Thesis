{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:22:10.919876Z",
     "iopub.status.busy": "2023-05-08T13:22:10.919429Z",
     "iopub.status.idle": "2023-05-08T13:22:11.249085Z",
     "shell.execute_reply": "2023-05-08T13:22:11.248040Z",
     "shell.execute_reply.started": "2023-05-08T13:22:10.919844Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:22:12.811128Z",
     "iopub.status.busy": "2023-05-08T13:22:12.810768Z",
     "iopub.status.idle": "2023-05-08T13:22:12.833939Z",
     "shell.execute_reply": "2023-05-08T13:22:12.832737Z",
     "shell.execute_reply.started": "2023-05-08T13:22:12.811100Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class GridworldEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a custom made Gym environment of a simple Gridworld, a NxN matrix where the agent starts from a starting position I and \n",
    "    has to reach the goal position G.\n",
    "    \n",
    "    The set of states S = {[x,y]| x,y ∈ [0, ..., grid_size]} or {}, represent the possible positions in the environment.\n",
    "    The set of actions A = [up, down, left, right].\n",
    "    The transition to a new state T(s_t+1 | s_t, a_t).\n",
    "    The reward R(s, a) = {1 if s == G else -0.1}.\n",
    "    \n",
    "    In this version the initial state is the position 0 ([0, 0]), I = [0, 0]. The goal state is the position 24 ([4, 4]) G = [4,4]. \n",
    "    \n",
    "    Args:\n",
    "    - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
    "    - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
    "    - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.state_space = spaces.Discrete(self.grid_size ** 2)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.reward_range = (-0.1, 1.0)\n",
    "        self.goal = (grid_size-1, grid_size-1)\n",
    "        self.current_pos = (0, 0)\n",
    "        self.done = False\n",
    "        self.time_horizon = time_horizon\n",
    "        self.steps_taken = 0\n",
    "        self.prob = prob\n",
    "        self.transition_matrix = self._build_transition_matrix()\n",
    "        \n",
    "    def _build_transition_matrix(self):\n",
    "        '''\n",
    "        This method builds the transition matrix for the MDP, T(s'|a, s).\n",
    "        The function should be used with the following order of operands:\n",
    "         - first parameter: s, the state where the agent takes the action in\n",
    "         - second parameter: s', the state where the agent arrives taking action a from state s\n",
    "         - third parameter: a, the action taken by the agent\n",
    "        '''\n",
    "        transition_matrix = np.zeros((self.grid_size**2, self.grid_size**2, self.action_space.n))\n",
    "        # For every state (i,j)\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                s = (i, j)\n",
    "                # For every action 'a' chosen as action from the agent\n",
    "                for a in range(self.action_space.n):\n",
    "                    # For all actions 'a_'\n",
    "                    for a_ in range(self.action_space.n):\n",
    "                        # Calculate the probability of \n",
    "                        prob = 1 - self.prob if a_ == a else self.prob / (self.action_space.n - 1)\n",
    "                        s_ = self._sample_new_position(a_, s)\n",
    "                        transition_matrix[self._state_to_index(s), self._state_to_index(s_), a] += prob\n",
    "        return transition_matrix\n",
    "    \n",
    "    def _sample_new_position(self, action, state):\n",
    "        if action == 0:  # up\n",
    "            new_pos = (state[0], max(0, state[1]-1))\n",
    "        elif action == 1:  # down\n",
    "            new_pos = (state[0], min(self.grid_size-1, state[1]+1))\n",
    "        elif action == 2:  # left\n",
    "            new_pos = (max(0, state[0]-1), state[1])\n",
    "        elif action == 3:  # right\n",
    "            new_pos = (min(self.grid_size-1, state[0]+1), state[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        return new_pos\n",
    "    \n",
    "    def _state_to_index(self, state):\n",
    "        return state[0] + state[1] * self.grid_size\n",
    "\n",
    "    def _index_to_state(self, index):\n",
    "        \"\"\"Converts an index to a state tuple (i, j).\"\"\"\n",
    "        i = index % self.grid_size\n",
    "        j = index // self.grid_size\n",
    "        #print(\"Converting %d => (%d, %d)\" %(index, i, j))\n",
    "        return (i, j)\n",
    "    \n",
    "    def sample_next_state(self, action):\n",
    "        current_state = self._state_to_index(self.current_pos)\n",
    "        action_probabilities = self.transition_matrix[current_state, :, action]\n",
    "        next_state_index = np.random.choice(self.grid_size**2, p=action_probabilities)\n",
    "        next_pos = self._index_to_state(next_state_index)\n",
    "        #print(\"Taking action %d from cur_pos index:%d (%d, %d): going to index %d which is state => (%d, %d)\" %(action, current_state, self.current_pos[0], self.current_pos[1], next_state_index, next_pos[0], next_pos[1]))\n",
    "        return next_pos\n",
    "        \n",
    "    def step(self, action):\n",
    "        #print(\"Timestep: %d\" %self.steps_taken)\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has already ended.\")\n",
    "        new_pos = self.sample_next_state(action)\n",
    "        reward = -0.1  # default reward for moving\n",
    "        if new_pos == self.goal:\n",
    "            reward = 1.0\n",
    "            self.done = True\n",
    "        self.current_pos = new_pos\n",
    "        self.steps_taken += 1\n",
    "        if self.time_horizon != -1 and self.steps_taken >= self.time_horizon:\n",
    "            self.done = True\n",
    "        return self.current_pos, reward, self.done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_pos = (0, 0)\n",
    "        self.done = False\n",
    "        self.steps_taken = 0\n",
    "        return self.current_pos\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            for j in range(self.grid_size):\n",
    "                for i in range(self.grid_size):\n",
    "                    if (i, j) == self.current_pos:\n",
    "                        print(\"X \", end='')\n",
    "                    elif (i, j) == self.goal:\n",
    "                        print(\"G \", end='')\n",
    "                    else:\n",
    "                        print(\"_ \", end='')\n",
    "                print()\n",
    "            print()\n",
    "            \n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                states.append((i, j))\n",
    "        return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Reinforce Agent MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:22:16.343128Z",
     "iopub.status.busy": "2023-05-08T13:22:16.342767Z",
     "iopub.status.idle": "2023-05-08T13:22:16.358867Z",
     "shell.execute_reply": "2023-05-08T13:22:16.357636Z",
     "shell.execute_reply.started": "2023-05-08T13:22:16.343101Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the REINFORCE agent\n",
    "class REINFORCEAgent():\n",
    "    '''\n",
    "    This class is the implementation of a REINFORCE agent that tries to maximize the objective function J(θ)=E_(τ ~ p_π)[R(τ)]\n",
    "    Args:\n",
    "     - env: the instance of the environment on which the agent is acting.\n",
    "     - alpha: the value of the learning rate to compute the policy update_single_sampling.\n",
    "     - gamma: the value of the discount for the reward in order to compute the discounted reward.\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.policy_params = np.zeros((env.grid_size, env.grid_size, env.action_space.n))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Sample action from the policy\n",
    "        params = self.policy_params[state[0], state[1], :]\n",
    "        probs = np.exp(params) / np.sum(np.exp(params))\n",
    "        action = np.random.choice(len(probs), p=probs)\n",
    "        return action\n",
    "\n",
    "    def compute_returns(self, episode):\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for t in reversed(range(len(episode))):\n",
    "            _, _, reward, _ = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            returns.append(G)\n",
    "        returns = np.array(list(reversed(returns)))\n",
    "        return returns\n",
    "\n",
    "    def update_single_sampling(self, episode):\n",
    "        '''\n",
    "        This version of the update_single_sampling is the Monte Carlo sampling version of the REINFORCE algorithm.\n",
    "        \n",
    "        Args:\n",
    "         - episode: the sampled trajectory from which we compute the policy gradient.\n",
    "        '''\n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(episode)\n",
    "        \n",
    "        # Compute the policy gradient\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, _, _ = episode[t]\n",
    "            params = self.policy_params[state[0], state[1], :]\n",
    "            probs = np.exp(params) / np.sum(np.exp(params))\n",
    "            dlogp = np.zeros_like(params)\n",
    "            for i in range(env.action_space.n):\n",
    "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "            grad[state[0], state[1], :] += dlogp * returns[t]\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        \n",
    "    def update_multiple_sampling(self, trajectories):\n",
    "        '''\n",
    "        This version of the update_single_sampling takes into consideration the approximation of the gradient approach instead of the single sampling. Instead\n",
    "        of working with only one trajectory it works with multiple trajectories in order to have a more accurate representation of the expected\n",
    "        value of the ∇J(θ).\n",
    "        \n",
    "        Args:\n",
    "         - trajectories: a list of sampled trajectories from which we compute the policy gradient.\n",
    "         \n",
    "        '''\n",
    "        # Compute the policy gradient\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for episode in trajectories:\n",
    "            for t in range(len(episode)):\n",
    "                returns = self.compute_returns(episode)\n",
    "                state, action, _, _ = episode[t]\n",
    "                # Compute the policy gradient\n",
    "                params = self.policy_params[state[0], state[1], :]\n",
    "                probs = np.exp(params) / np.sum(np.exp(params))\n",
    "                dlogp = np.zeros_like(params)\n",
    "                for i in range(env.action_space.n):\n",
    "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "                grad[state[0], state[1], :] += dlogp * returns[t]\n",
    "        grad /= len(trajectories)\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test single sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T14:15:38.497239Z",
     "iopub.status.busy": "2023-05-06T14:15:38.496346Z",
     "iopub.status.idle": "2023-05-06T14:17:08.985397Z",
     "shell.execute_reply": "2023-05-06T14:17:08.984397Z",
     "shell.execute_reply.started": "2023-05-06T14:15:38.497191Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Gridworld environment and the REINFORCE agent\n",
    "trajectories = 1000\n",
    "env = GridworldEnv(time_horizon=200)\n",
    "agent = REINFORCEAgent(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_rewards = []\n",
    "avg_rewards = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    agent.update_single_sampling(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards[-trajectories:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "    ax.clear()\n",
    "    ax.plot(total_rewards, label='Total Reward')\n",
    "    ax.plot(avg_rewards, label='Average Reward (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test gradient approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T14:17:08.987292Z",
     "iopub.status.busy": "2023-05-06T14:17:08.986975Z",
     "iopub.status.idle": "2023-05-06T14:19:00.369822Z",
     "shell.execute_reply": "2023-05-06T14:19:00.368803Z",
     "shell.execute_reply.started": "2023-05-06T14:17:08.987267Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a10b19038e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_gradient_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtotal_avg_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_traj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mavg_avg_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_avg_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mavg_avg_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_avg_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Gridworld environment and the REINFORCE agent\n",
    "trajectories = 1000\n",
    "n_traj = 20\n",
    "env = GridworldEnv(time_horizon=100)\n",
    "agent = REINFORCEAgent(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_avg_rewards = []\n",
    "avg_avg_rewards = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    trajectories = []\n",
    "    total_rewards = []\n",
    "    for k in range(n_traj):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        trajectories.append(episode)\n",
    "    agent.update_multiple_sampling(trajectories)\n",
    "    total_avg_rewards.append(np.mean(total_rewards[-n_traj:]))\n",
    "    avg_avg_reward = np.mean(total_avg_rewards[-trajectories:])\n",
    "    avg_avg_rewards.append(avg_avg_reward)\n",
    "    ax.clear()\n",
    "    ax.plot(total_avg_rewards, label='Total Reward')\n",
    "    ax.plot(avg_avg_rewards, label='Average Reward (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Reinforce Agent MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:22:22.387188Z",
     "iopub.status.busy": "2023-05-08T13:22:22.386836Z",
     "iopub.status.idle": "2023-05-08T13:22:22.404243Z",
     "shell.execute_reply": "2023-05-08T13:22:22.403115Z",
     "shell.execute_reply.started": "2023-05-08T13:22:22.387162Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgentE(REINFORCEAgent):\n",
    "    '''\n",
    "    This class is the extension of the previous Reinforce agent. The only change is the objective followed by the agent that this time is going\n",
    "    to be the state visitation entropy J(θ) = E_(τ ~ p_π)[H(d_τ(s))] .\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.01, gamma=0.8):\n",
    "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
    "        # Optional initialization to have a non-uniform initial policy\n",
    "        self.policy_params = self.create_dummy_policy()\n",
    "        \n",
    "    def create_dummy_policy(self):\n",
    "        params = np.zeros((env.grid_size, env.grid_size, env.action_space.n))\n",
    "        for i in range(self.env.grid_size):\n",
    "            for j in range(self.env.grid_size):\n",
    "                params[i,j,0] = 1\n",
    "        return params\n",
    "    \n",
    "    def compute_state_distribution(self, episode, t):\n",
    "        d_t = np.zeros((env.grid_size, env.grid_size))\n",
    "        for t in range (t + 1):\n",
    "            _, _ , _, s_t = episode[t]\n",
    "            d_t[s_t[0]][s_t[1]] += 1\n",
    "        d_t /= (t + 1)\n",
    "        return d_t\n",
    "\n",
    "    def update_single_sampling(self, episode):\n",
    "        # Compute the t-step state distribution for the current policy\n",
    "        d_t = self.compute_state_distribution(episode, len(episode) - 1)\n",
    "        # Compute entropy of d_t\n",
    "        entropy = 0\n",
    "        for row in d_t:\n",
    "            for s in row:\n",
    "                if s != 0:\n",
    "                    entropy -= s * np.log(s)\n",
    "        \n",
    "        # Update policy parameters using the gradient of the entropy of the t-step state distribution\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        for t in range(len(episode)):\n",
    "            state, action, _, _ = episode[t]\n",
    "            # Compute the policy gradient\n",
    "            params = self.policy_params[state[0], state[1], :]\n",
    "            probs = np.exp(params) / np.sum(np.exp(params))\n",
    "            dlogp = np.zeros_like(params)\n",
    "            for i in range(env.action_space.n):\n",
    "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "            grad[state[0], state[1], :] += dlogp \n",
    "        grad *= entropy\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        return entropy, d_t\n",
    "    \n",
    "    def update_multiple_sampling(self, trajectories):\n",
    "        # Update policy parameters using the approximated gradient of the entropy objective function\n",
    "        grad = np.zeros_like(self.policy_params)\n",
    "        entropies = []\n",
    "        for episode in trajectories:\n",
    "            # Initialize the gradient of the k-th sampled trajectory\n",
    "            grad_k = np.zeros_like(self.policy_params)\n",
    "            # Compute the t-step state distribution for the current policy\n",
    "            d_t = self.compute_state_distribution(episode, len(episode) - 1)\n",
    "            # Compute entropy of d_t\n",
    "            entropy = 0\n",
    "            for row in d_t:\n",
    "                for s in row:\n",
    "                    if s != 0:\n",
    "                        entropy -= s * np.log(s)\n",
    "            for t in range(len(episode)):\n",
    "                state, action, _, _ = episode[t]\n",
    "                # Compute the policy gradient\n",
    "                params = self.policy_params[state[0], state[1], :]\n",
    "                probs = np.exp(params) / np.sum(np.exp(params))\n",
    "                dlogp = np.zeros_like(params)\n",
    "                for i in range(env.action_space.n):\n",
    "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
    "                grad_k[state[0], state[1], :] += dlogp \n",
    "            grad_k *= entropy\n",
    "            # Sum the k-th gradient to the final gradient\n",
    "            grad += grad_k\n",
    "            entropies.append(entropy)\n",
    "        # Divide the gradient by the number of trajectories sampled\n",
    "        grad /= len(trajectories)\n",
    "        # Update the policy parameters\n",
    "        self.policy_params += self.alpha * grad\n",
    "        return entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test single sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:22:25.688231Z",
     "iopub.status.busy": "2023-05-08T13:22:25.687051Z",
     "iopub.status.idle": "2023-05-08T13:24:15.953754Z",
     "shell.execute_reply": "2023-05-08T13:24:15.952900Z",
     "shell.execute_reply.started": "2023-05-08T13:22:25.688169Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "trajectories = 1000\n",
    "env = GridworldEnv(time_horizon = 200, prob = 0.1)\n",
    "agent = REINFORCEAgentE(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_rewards = []\n",
    "avg_rewards = []\n",
    "entropies = []\n",
    "avg_entropies = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(trajectories):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    #print(\"start episode %d\" %i)\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, action, reward, next_state))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    #print(\"F\")\n",
    "    state = env.reset()\n",
    "    entropy, d_t = agent.update_single_sampling(episode)\n",
    "    '''\n",
    "    if i % 100 == 0:\n",
    "        print(d_t)\n",
    "    '''\n",
    "    entropies.append(entropy)\n",
    "    total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards[-trajectories:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "    avg_entropy = np.mean(entropies[-trajectories:])\n",
    "    avg_entropies.append(avg_entropy)\n",
    "    \n",
    "    #Plot Reward and Entropy\n",
    "    ax.clear()\n",
    "    #ax.plot(total_rewards, label='Total Reward')\n",
    "    #ax.plot(avg_rewards, label='Average Reward (%d episodes)' %episodes)\n",
    "    ax.plot(entropies, label='Entropy')\n",
    "    ax.plot(avg_entropies, label='Average Entropy (%d episodes)' %trajectories)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward/Entropy')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test gradient approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T15:14:04.667251Z",
     "iopub.status.busy": "2023-05-06T15:14:04.666820Z",
     "iopub.status.idle": "2023-05-06T15:27:18.157204Z",
     "shell.execute_reply": "2023-05-06T15:27:18.156114Z",
     "shell.execute_reply.started": "2023-05-06T15:14:04.667219Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "n_traj = 50\n",
    "env = GridworldEnv(time_horizon = 200, prob = 0)\n",
    "agent = REINFORCEAgentE(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "avg_entropies = []\n",
    "avg_avg_entropies = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(episodes):\n",
    "    trajectories = []\n",
    "    entropies = []\n",
    "    for k in range(n_traj):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        trajectories.append(episode)\n",
    "    entropies = agent.update_multiple_sampling(trajectories)\n",
    "    avg_entropies.append(np.mean(entropies[-n_traj:]))\n",
    "    avg_avg_entropy = np.mean(avg_entropies[-episodes:])\n",
    "    avg_avg_entropies.append(avg_avg_entropy)\n",
    "\n",
    "    #Plot Reward and Entropy\n",
    "    ax.clear()\n",
    "    ax.plot(avg_entropies, label='Entropy')\n",
    "    ax.plot(avg_avg_entropies, label='Average Entropy (%d episodes)' %episodes)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward/Entropy')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T13:35:46.805859Z",
     "iopub.status.busy": "2023-05-08T13:35:46.805454Z",
     "iopub.status.idle": "2023-05-08T13:35:46.819136Z",
     "shell.execute_reply": "2023-05-08T13:35:46.818103Z",
     "shell.execute_reply.started": "2023-05-08T13:35:46.805828Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class GridworldPOMDPEnv(GridworldEnv):\n",
    "    '''\n",
    "    This class implements the extension of the upper Gridworld MDP, making it become a POMDP by inserting the observation mechanism.\n",
    "    The observations here are modeled as a probability of returning the real state of the MDP, so the agent can see the real state with\n",
    "    probability obs_prob and the others with probability 1 - obs_prob.\n",
    "    \n",
    "    Args:\n",
    "     - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
    "     - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
    "     - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
    "     - obs_prob: the probability with which the environment gives back the actual state \n",
    "    '''\n",
    "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, obs_prob = 0.8):\n",
    "        # Initialize the underlying Gridworld MDP\n",
    "        super().__init__(grid_size=grid_size, time_horizon=time_horizon, prob=prob)\n",
    "        # Initialize all the POMDP specific variables\n",
    "        self.observation_space = spaces.Discrete(self.grid_size**2)\n",
    "        self.obs_prob = obs_prob\n",
    "        self.observation_matrix = self.build_observation_matrix()\n",
    "        \n",
    "    def build_observation_matrix(self):\n",
    "        '''\n",
    "        This method creates the observation matrix for our environment.\n",
    "        The observation function indexes are to be used in order:\n",
    "         - first param: true state s\n",
    "         - second param: observation o\n",
    "        '''\n",
    "        # Initialize the observation function with zeros\n",
    "        observation_matrix = np.zeros((self.grid_size**2, self.grid_size**2))\n",
    "        for s_i in range(self.observation_space.n):\n",
    "            for s in range(self.observation_space.n):\n",
    "                # For every state, put probabilty to obs_prob if the outer state is equal to the inner, otherwise to the uniform between the other states.\n",
    "                observation_matrix[s_i][s] += self.obs_prob if s == s_i else (1 - self.obs_prob) / (self.observation_space.n - 1)\n",
    "        # Return the built matrix\n",
    "        return observation_matrix\n",
    "\n",
    "    def reset(self):\n",
    "        # Call the upper reset of the environment\n",
    "        super().reset()\n",
    "        # Set the initial belief to a uniform distribution and give it to the agent\n",
    "        initial_belief = np.ones(self.observation_space.n) / self.observation_space.n\n",
    "        return initial_belief\n",
    "\n",
    "    def step(self, action):\n",
    "        # Make the step of the underlying MDP\n",
    "        true_state, reward, done, info = super().step(action)\n",
    "        # Get the index of the state\n",
    "        true_state_index = self._state_to_index(true_state)\n",
    "        # Get the observation probabilities for the state\n",
    "        obs_prob = self.observation_matrix[true_state_index]\n",
    "        # Sample the next observation from the probabilities\n",
    "        obs = np.random.choice(self.grid_size**2, p=obs_prob)\n",
    "        # Change index to position\n",
    "        obs = self._index_to_state(obs)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Reinforce POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T14:13:01.943633Z",
     "iopub.status.busy": "2023-05-08T14:13:01.942624Z",
     "iopub.status.idle": "2023-05-08T14:13:01.956000Z",
     "shell.execute_reply": "2023-05-08T14:13:01.954762Z",
     "shell.execute_reply.started": "2023-05-08T14:13:01.943558Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgentPOMDP(REINFORCEAgent):\n",
    "    '''\n",
    "    This calls implements the Reinforce Agent for the POMDP defined above.\n",
    "    It is implemented as the extension of the former MDP agent but adds the belief state and what is concerned by it.\n",
    "    '''\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99):\n",
    "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
    "        self.belief = np.ones(env.grid_size**2)/(env.grid_size * env.grid_size)\n",
    "        self.last_sampled_state = 0\n",
    "        self.n_expected_value = 50\n",
    "    \n",
    "    # TODO: testino funzionamento\n",
    "    def belief_update(self, action, observation):\n",
    "        '''\n",
    "        This method updates the belief of the agent in a Bayesian way. \n",
    "        '''\n",
    "        # Get the index of the observed state\n",
    "        obs_state_index = self.env._state_to_index(observation)\n",
    "        # Initialize the updated belief vector\n",
    "        unnormalized_updated_belief = np.zeros(self.belief.shape)\n",
    "        for s in range(env.state_space.n):\n",
    "            # Get O(o\\s)\n",
    "            obs_o_s = self.env.observation_matrix[s, obs_state_index]\n",
    "            # Calculate the sum over all states\n",
    "            summation = np.sum(self.env.transition_matrix[s, :, action] * self.belief, axis=0)\n",
    "            b_s = obs_o_s * summation\n",
    "            unnormalized_updated_belief[s] = b_s\n",
    "        # Normalize the belief and update_single_sampling it\n",
    "        self.belief = unnormalized_updated_belief / np.sum(unnormalized_updated_belief)\n",
    "        \n",
    "    def get_state(self, mode):\n",
    "        '''\n",
    "        This method is used to sample or compute the expected state given the current belief.\n",
    "        Args:\n",
    "         - mode: if 0 it does a single sampling of the state.\n",
    "                 if 1 it gives the approximated expected value.\n",
    "                 No other values allowed.\n",
    "        '''\n",
    "        if mode != 0 and mode != 1:\n",
    "            raise Exception(\"You have to pass me 0 or 1, read :/\")\n",
    "        \n",
    "        if mode == 0:\n",
    "            state = np.random.choice(self.belief.size, p=self.belief)\n",
    "            self.last_sampled_state = state\n",
    "            state = env._index_to_state(state)\n",
    "            return state\n",
    "        \n",
    "        if mode == 1:\n",
    "            states = []\n",
    "            for i in range(self.n_expected_value):\n",
    "                states.append(np.random.choice(self.belief.size, p=self.belief))\n",
    "            state = max(set(states), key = states.count)\n",
    "            self.last_sampled_state = state\n",
    "            state = env._index_to_state(state)\n",
    "            return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini Test Belief Update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "env = GridworldPOMDPEnv(time_horizon=100)\n",
    "agent = REINFORCEAgentPOMDP(env)\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T14:13:04.836641Z",
     "iopub.status.busy": "2023-05-08T14:13:04.835996Z",
     "iopub.status.idle": "2023-05-08T14:14:00.789348Z",
     "shell.execute_reply": "2023-05-08T14:14:00.788195Z",
     "shell.execute_reply.started": "2023-05-08T14:13:04.836604Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the Gridworld environment and the REINFORCE agent\n",
    "episodes = 500\n",
    "env = GridworldPOMDPEnv(time_horizon=100)\n",
    "agent = REINFORCEAgentPOMDP(env)\n",
    "\n",
    "# Train the agent and plot the rewards\n",
    "total_rewards = []\n",
    "avg_rewards = []\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(episodes):\n",
    "    episode = []\n",
    "    agent.belief = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        sampled_state = agent.get_state(1)\n",
    "        action = agent.get_action(sampled_state)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        episode.append((sampled_state, action, reward, next_obs))\n",
    "        agent.belief_update(action, next_obs)\n",
    "        total_reward += reward\n",
    "    agent.update_single_sampling(episode)\n",
    "    total_rewards.append(total_reward)\n",
    "    avg_reward = np.mean(total_rewards[-episodes:])\n",
    "    avg_rewards.append(avg_reward)\n",
    "    ax.clear()\n",
    "    ax.plot(total_rewards, label='Total Reward')\n",
    "    ax.plot(avg_rewards, label='Average Reward (%d episodes)' %episodes)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Reinforce POMDP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
