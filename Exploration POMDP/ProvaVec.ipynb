{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import t, mode\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JEMjPu_sD_G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_mESbPTDwN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf8a88f-03fe-4e60-8d3c-4d0a30a9429e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def print_gridworld_with_policy(agent, figsize=(6, 6), title=\"notitle\"):\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    for state in range(agent.env.state_space.n):\n",
        "        col, row = agent.env.index_to_state(state)\n",
        "\n",
        "        plt.gca().add_patch(\n",
        "            plt.Rectangle((col, agent.env.grid_size - row - 1), 1, 1, facecolor='white', edgecolor='black'))\n",
        "\n",
        "        for a in range(agent.env.action_space.n):\n",
        "            prob = np.exp(agent.policy_params[state, a]) / np.sum(np.exp(agent.policy_params[state]))\n",
        "            # Calculate the center coordinates of the state cell\n",
        "            center_x = col + 0.5\n",
        "            center_y = agent.env.grid_size - row - 0.5\n",
        "\n",
        "            # Calculate the arrow starting coordinates relative to the center\n",
        "            arrow_start_x = center_x\n",
        "            arrow_start_y = center_y\n",
        "\n",
        "            # Calculate the arrow ending coordinates relative to the center based on the action\n",
        "            if a == 0:  # Up arrow\n",
        "                arrow_end_x = center_x\n",
        "                arrow_end_y = center_y + prob * 0.3  # Adjust the scaling factor to control the arrow length\n",
        "            elif a == 1:  # Down arrow\n",
        "                arrow_end_x = center_x\n",
        "                arrow_end_y = center_y - prob * 0.3\n",
        "            elif a == 2:  # Left arrow\n",
        "                arrow_end_x = center_x - prob * 0.3\n",
        "                arrow_end_y = center_y\n",
        "            elif a == 3:  # Right arrow\n",
        "                arrow_end_x = center_x + prob * 0.3\n",
        "                arrow_end_y = center_y\n",
        "\n",
        "            # Calculate the arrowhead size based on the value\n",
        "            head_width = prob * 0.2  # Adjust the scaling factor to control the arrowhead width\n",
        "            head_length = prob * 0.2  # Adjust the scaling factor to control the arrowhead length\n",
        "\n",
        "            # Draw the arrow\n",
        "            arrow_thickness = 1  # Adjust the scaling factor to control the arrow thickness\n",
        "            plt.arrow(arrow_start_x, arrow_start_y, arrow_end_x - arrow_start_x, arrow_end_y - arrow_start_y,\n",
        "                      head_width=head_width, head_length=head_length, fc='black', ec='black', linewidth=arrow_thickness)\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def print_heatmap(agent, d_t, title):\n",
        "    '''\n",
        "    This method serves the purpose of printing a heatmap of the state distribution given a trajectory of states drawn by\n",
        "    an agent acting inside an environment.\n",
        "    :param agent: the agent that acted in the environment;\n",
        "    :param d_t: the state visitation distribution.\n",
        "    '''\n",
        "    # Create the subplot\n",
        "    ax = plt.gca()\n",
        "    # Check corridor\n",
        "    if hasattr(agent.env, 'length_corridor'):\n",
        "        heatmap_data = np.full((agent.env.grid_size + agent.env.length_corridor, agent.env.grid_size), np.nan)\n",
        "    else:\n",
        "        heatmap_data = np.full((agent.env.grid_size, agent.env.grid_size), np.nan)\n",
        "    # Prepare data for plotting\n",
        "    for index in range(agent.env.state_space.n):\n",
        "        col, row = agent.env.index_to_state(index)\n",
        "        heatmap_data[row][col] = d_t[index]\n",
        "    # Plot the heatmap\n",
        "    sns.heatmap(data=heatmap_data, annot=True, ax=ax)\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_graph(n_run, n_episodes, list_entropies, list_true_entropies, confidence):\n",
        "    # Initialize the plotting vectors\n",
        "    fig, ax = plt.subplots(figsize=(30, 10))\n",
        "    plt.title(\"Big number of episodes normal learning rate, normal gaussian\")\n",
        "    entropies_means = []\n",
        "    true_entropies_means = []\n",
        "    under_line = []\n",
        "    over_line = []\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        entropies_mean = np.mean(list_entropies[i])\n",
        "        true_entropies_mean = np.mean(list_true_entropies[i])\n",
        "        entropies_std = np.std(list_entropies[i])\n",
        "        freedom_deg = n_run - 1\n",
        "        t_crit = np.abs(t.ppf((1 - confidence) / 2, freedom_deg))\n",
        "        entropies_means.append(entropies_mean)\n",
        "        true_entropies_means.append(true_entropies_mean)\n",
        "        under_line.append(entropies_mean - entropies_std * t_crit / np.sqrt(n_run))\n",
        "        over_line.append(entropies_mean + entropies_std * t_crit / np.sqrt(n_run))\n",
        "\n",
        "    ax.clear()\n",
        "    ax.plot(entropies_means, label='Believed Entropy')\n",
        "    ax.plot(true_entropies_means, label='True Entropy')\n",
        "    ax.fill_between(np.arange(n_episodes), under_line, over_line, color='b', alpha=0.1)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Entropy')\n",
        "    ax.legend()\n",
        "    fig.canvas.draw()\n",
        "    return entropies_means, true_entropies_means"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corridor Gridworld MDP"
      ],
      "metadata": {
        "id": "tosPVpYMEIKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    \"\"\"\n",
        "    This class implements a custom made Gym environment of a simple Gridworld, a NxN matrix where the agent starts from a starting position I, with an\n",
        "    additional corridor situated in the right bottom corner.\n",
        "\n",
        "    The set of states S = {[x,y]| x,y ∈ [0, ..., grid_size]} or {}, represent the possible positions in the environment.\n",
        "    The set of actions A = [up, down, left, right].\n",
        "    The transition to a new state T(s_t+1 | s_t, a_t).\n",
        "    The reward R(s, a) = {1 if s == G else -0.1}.\n",
        "\n",
        "    In this version the initial state is the position 0 ([0, 0]), I = [0, 0]. The goal state is the position 24 ([4, 4]) G = [4,4].\n",
        "\n",
        "    Args:\n",
        "    - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
        "    - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
        "    - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
        "    - length_corridor: the length of the corridor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, randomize=0, length_corridor=3):\n",
        "        self.grid_size = grid_size\n",
        "        self.length_corridor = length_corridor\n",
        "        self.state_space = spaces.Discrete(self.grid_size ** 2 + self.length_corridor)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.reward_range = (-0.1, 1.0)\n",
        "        self.goal = (grid_size-1, grid_size-1 + length_corridor)\n",
        "        self.current_pos = (0, length_corridor + grid_size - 1)\n",
        "        self.done = False\n",
        "        self.time_horizon = time_horizon\n",
        "        self.steps_taken = 0\n",
        "        self.prob = prob\n",
        "        self.transition_matrix = self._build_transition_matrix()\n",
        "        self.randomize = randomize\n",
        "        if self.randomize == 1:\n",
        "            self.current_pos = self.index_to_state(np.random.randint(0, self.state_space.n))\n",
        "        else:\n",
        "            self.current_pos = (0, 0)\n",
        "\n",
        "    def _build_transition_matrix(self):\n",
        "        '''\n",
        "        This method builds the transition matrix for the MDP, T(s'|a, s).\n",
        "        The function should be used with the following order of operands:\n",
        "         - first parameter: s, the state where the agent takes the action in\n",
        "         - second parameter: s', the state where the agent arrives taking action a from state s\n",
        "         - third parameter: a, the action taken by the agent\n",
        "        '''\n",
        "        transition_matrix = np.zeros((self.state_space.n, self.state_space.n, self.action_space.n))\n",
        "        # For every state (i,j)\n",
        "        for index in range(self.state_space.n):\n",
        "            s = self.index_to_state(index)\n",
        "            # For every action 'a' chosen as action from the agent\n",
        "            for a in range(self.action_space.n):\n",
        "                # For all actions 'a_'\n",
        "                for a_ in range(self.action_space.n):\n",
        "                    # Calculate the probability of\n",
        "                    prob = 1 - self.prob if a_ == a else self.prob / (self.action_space.n - 1)\n",
        "                    s_ = self._sample_new_position(a_, s)\n",
        "                    transition_matrix[self.state_to_index(s_), self.state_to_index(s), a] += prob\n",
        "        return transition_matrix\n",
        "\n",
        "    def _sample_new_position(self, action, state):\n",
        "        if action == 0:  # up\n",
        "            if state[1] >= self.grid_size:\n",
        "                new_pos = (state[0], state[1] - 1) # if you are in the corridor you just go one y less\n",
        "            else:\n",
        "                new_pos = (state[0], max(0, state[1]-1))\n",
        "        elif action == 1:  # down\n",
        "            if state == (self.grid_size - 1, self.grid_size - 1) or state[1] >= self.grid_size:\n",
        "                new_pos = (state[0], min(self.grid_size + self.length_corridor - 1, state[1] + 1)) # if you are in the corridor, or you are entering the corridor, you add one unless you are at the end of the corridor\n",
        "            else:\n",
        "                new_pos = (state[0], min(self.grid_size-1, state[1]+1))\n",
        "        elif action == 2:  # left\n",
        "            if state[1] >= self.grid_size:\n",
        "                new_pos = state # if you are in the corridor you don't move\n",
        "            else:\n",
        "                new_pos = (max(0, state[0]-1), state[1])\n",
        "        elif action == 3:  # right\n",
        "            if state[1] >= self.grid_size:\n",
        "                new_pos = state # if you are in the corridor you don't move\n",
        "            else:\n",
        "                new_pos = (min(self.grid_size-1, state[0]+1), state[1])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action.\")\n",
        "        return new_pos\n",
        "\n",
        "    def state_to_index(self, state):\n",
        "        if state[1] >= self.grid_size:\n",
        "            return self.grid_size ** 2 + state[1] - self.grid_size\n",
        "        return state[0] + state[1] * self.grid_size\n",
        "\n",
        "    def index_to_state(self, index):\n",
        "        \"\"\"Converts an index to a state tuple (i, j).\"\"\"\n",
        "        if index >= self.grid_size ** 2:\n",
        "            i = self.grid_size - 1\n",
        "            j = self.grid_size + index - self.grid_size ** 2\n",
        "            return (i, j)\n",
        "        i = index % self.grid_size\n",
        "        j = index // self.grid_size\n",
        "        #print(\"Converting %d => (%d, %d)\" %(index, i, j))\n",
        "        return (i, j)\n",
        "\n",
        "    def sample_next_state(self, action):\n",
        "        current_state = self.state_to_index(self.current_pos)\n",
        "        action_probabilities = self.transition_matrix[:, current_state, action]\n",
        "        next_state_index = np.random.choice(self.state_space.n, p=action_probabilities)\n",
        "        next_pos = self.index_to_state(next_state_index)\n",
        "        #print(\"Taking action %d from cur_pos index:%d (%d, %d): going to index %d which is state => (%d, %d)\" %(action, current_state, self.current_pos[0], self.current_pos[1], next_state_index, next_pos[0], next_pos[1]))\n",
        "        return next_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        #print(\"Time-step: %d\" %self.steps_taken)\n",
        "        if self.done:\n",
        "            raise ValueError(\"Episode has already ended.\")\n",
        "        new_pos = self.sample_next_state(action)\n",
        "        reward = -0.1  # default reward for moving\n",
        "        if new_pos == self.goal:\n",
        "            reward = 1.0\n",
        "            self.done = True\n",
        "        self.current_pos = new_pos\n",
        "        self.steps_taken += 1\n",
        "        if self.time_horizon != -1 and self.steps_taken >= self.time_horizon:\n",
        "            self.done = True\n",
        "        return self.state_to_index(self.current_pos), reward, self.done, False, {}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "\n",
        "        if self.randomize == 1:\n",
        "            self.current_pos = self.index_to_state(np.random.randint(0, self.state_space.n))\n",
        "        else:\n",
        "            self.current_pos = (0, 0)\n",
        "        self.done = False\n",
        "        self.steps_taken = 0\n",
        "        return self.state_to_index(self.current_pos)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if mode == 'human':\n",
        "            for j in range(self.grid_size):\n",
        "                for i in range(self.grid_size):\n",
        "                    if (i, j) == self.current_pos:\n",
        "                        print(\"X \", end='')\n",
        "                    elif (i, j) == self.goal:\n",
        "                        print(\"G \", end='')\n",
        "                    else:\n",
        "                        print(\"_ \", end='')\n",
        "                print()\n",
        "            print()"
      ],
      "metadata": {
        "id": "7zK5hR6wEJoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldEnvGoalless(GridworldEnv):\n",
        "    '''\n",
        "    This extension of the environment work functionally like the previous, but it removes the ending goal state.\n",
        "    '''\n",
        "    def step(self, action):\n",
        "        #print(\"Time-step: %d\" %self.steps_taken)\n",
        "        if self.done:\n",
        "            raise ValueError(\"Episode has already ended.\")\n",
        "        new_pos = self.sample_next_state(action)\n",
        "        reward = -0.1  # default reward for moving\n",
        "        if new_pos == self.goal:\n",
        "            reward = 1.0\n",
        "        self.current_pos = new_pos\n",
        "        self.steps_taken += 1\n",
        "        if self.time_horizon != -1 and self.steps_taken >= self.time_horizon:\n",
        "            self.done = True\n",
        "        return self.state_to_index(self.current_pos), reward, self.done, {}"
      ],
      "metadata": {
        "id": "NMcNI5SZEM6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward Reinforce Agent MDP"
      ],
      "metadata": {
        "id": "7rWaQVYcEP6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the REINFORCE agent\n",
        "class REINFORCEAgent():\n",
        "    '''\n",
        "    This class is the implementation of a REINFORCE agent that tries to maximize the objective function J(θ)=E_(τ ~ p_π)[R(τ)]\n",
        "    Args:\n",
        "     - env: the instance of the environment on which the agent is acting.\n",
        "     - alpha: the value of the learning rate to compute the policy update.\n",
        "     - gamma: the value of the discount for the reward in order to compute the discounted reward.\n",
        "    '''\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9):\n",
        "        self.env = env\n",
        "        self.alpha = alpha  # learning rate\n",
        "        self.gamma = gamma  # discount factor\n",
        "        self.policy_params = np.zeros((env.state_space.n, env.action_space.n))\n",
        "        self.ohe_states = np.identity(env.state_space.n)\n",
        "\n",
        "    def get_probability(self, state):\n",
        "        # Compute the dot product\n",
        "        params = np.dot(state, self.policy_params)\n",
        "        # Compute the softmax\n",
        "        probs = np.exp(params) / np.sum(np.exp(params))\n",
        "        return probs\n",
        "\n",
        "    def compute_returns(self, episode):\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for t in reversed(range(len(episode))):\n",
        "            _, _, _, reward, _ = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            returns.append(G)\n",
        "        returns = np.array(list(reversed(returns)))\n",
        "        return returns\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # Get probability vector\n",
        "        probs = self.get_probability(state)\n",
        "        # Sample the action\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        return action, probs\n",
        "\n",
        "    def update_single_sampling(self, episode):\n",
        "        '''\n",
        "        This version of the update is the Monte Carlo sampling version of the REINFORCE algorithm.\n",
        "\n",
        "        Args:\n",
        "         - episode: the sampled trajectory from which we compute the policy gradient.\n",
        "        '''\n",
        "        # Compute returns\n",
        "        returns = self.compute_returns(episode)\n",
        "        # Compute the policy gradient\n",
        "        grad = np.zeros_like(self.policy_params)\n",
        "        for t in range(len(episode)):\n",
        "            state, action, probs, _, _ = episode[t]\n",
        "            dlogp = np.zeros(self.env.action_space.n)\n",
        "            for i in range(env.action_space.n):\n",
        "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
        "            grad += np.outer(state, dlogp) * returns[t]\n",
        "        # Update the policy parameters\n",
        "        self.policy_params += self.alpha * grad\n",
        "\n",
        "    def update_multiple_sampling(self, trajectories):\n",
        "        '''\n",
        "        This version of the update takes into consideration the approximation of the gradient by sampling multiple trajectories. Instead\n",
        "        of working with only one trajectory it works with multiple trajectories in order to have a more accurate representation of the expected\n",
        "        value of the ∇J(θ).\n",
        "\n",
        "        Args:\n",
        "         - trajectories: a list of sampled trajectories from which we compute the policy gradient.\n",
        "\n",
        "        '''\n",
        "        # Compute the policy gradient\n",
        "        grad = np.zeros_like(self.policy_params)\n",
        "        for episode in trajectories:\n",
        "            returns = self.compute_returns(episode)\n",
        "            for t in range(len(episode)):\n",
        "                state, action, probs, _, _ = episode[t]\n",
        "                dlogp = np.zeros(self.env.action_space.n)\n",
        "                for i in range(env.action_space.n):\n",
        "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
        "                grad += np.outer(state, dlogp) * returns[t]\n",
        "        grad /= len(trajectories)\n",
        "        # Update the policy parameters\n",
        "        self.policy_params += self.alpha * grad"
      ],
      "metadata": {
        "id": "IeeAvABpEPUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy Reinforce Agent MDP\n"
      ],
      "metadata": {
        "id": "tDjERKKPEcuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCEAgentE(REINFORCEAgent):\n",
        "    '''\n",
        "    This class is the extension of the previous Reinforce agent. The only change is the objective followed by the agent that this time is going\n",
        "    to be the state-action visitation entropy J(θ) = E_(τ ~ p_π)[H(d_τ(s,a))] .\n",
        "\n",
        "    Args:\n",
        "         - policy:\n",
        "            0: initialize the dummy policy;\n",
        "            1: initialize the optimal policy;\n",
        "            2: initialize the uniform policy.\n",
        "    '''\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, policy=-1):\n",
        "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
        "        # Optional initialization to have a non-uniform initial policy\n",
        "        self.init_policy(policy)\n",
        "\n",
        "    def init_policy(self, policy):\n",
        "        if policy == 0:\n",
        "            self.policy_params = self.create_dummy_policy()\n",
        "        elif policy == 1:\n",
        "            self.policy_params = self.create_optimal_policy()\n",
        "        else:\n",
        "            self.policy_params = np.ones((env.state_space.n, env.action_space.n))\n",
        "\n",
        "    def create_dummy_policy(self):\n",
        "        params = np.zeros((env.state_space.n, env.action_space.n))\n",
        "        for i in range(self.env.state_space.n):\n",
        "                params[i, 0] = 1\n",
        "        return params\n",
        "\n",
        "    def create_optimal_policy(self):\n",
        "        params = np.zeros((env.state_space.n, env.action_space.n))\n",
        "        k = 1\n",
        "        actions = [3,3,3,2]\n",
        "        for i in range(self.env.state_space.n):\n",
        "                if i % self.env.grid_size == self.env.grid_size - 1 and (k == 1):\n",
        "                    params[i, 1] = 500\n",
        "                    k += 1\n",
        "                elif i % self.env.grid_size == self.env.grid_size - 1 and (k == 3):\n",
        "                    params[i, actions[k]] = 500\n",
        "                    k = 0\n",
        "                elif i % self.env.grid_size == 0 and k == 2:\n",
        "                    params[i, 1] = 500\n",
        "                    k += 1\n",
        "                elif i % self.env.grid_size == 0 and k == 0:\n",
        "                    params[i, actions[k]] = 500\n",
        "                    k += 1\n",
        "                else:\n",
        "                    params[i, actions[k]] = 500\n",
        "        return params\n",
        "\n",
        "    def compute_entropy(self, d_t):\n",
        "        # Create the condition to apply the log\n",
        "        condition = d_t > 0\n",
        "        # Compute log of d_t\n",
        "        log_d_t = -np.log(d_t, where=condition)\n",
        "        # Compute the entropy\n",
        "        entropy = np.sum(np.multiply(d_t, log_d_t))\n",
        "        return entropy\n",
        "\n",
        "    def update_single_sampling(self, episode, d_t):\n",
        "        # Compute entropy of d_t\n",
        "        entropy = self.compute_entropy(d_t)\n",
        "        # Update policy parameters using the gradient of the entropy of the t-step state distribution\n",
        "        grad = np.zeros_like(self.policy_params)\n",
        "        for t in range(len(episode)):\n",
        "            state, action, probs, _, _ = episode[t]\n",
        "            # Compute the policy gradient\n",
        "            dlogp = np.zeros(self.env.action_space.n)\n",
        "            for i in range(env.action_space.n):\n",
        "                dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
        "            grad += np.outer(state, dlogp) * entropy\n",
        "        grad *= entropy\n",
        "        # Update the policy parameters\n",
        "        self.policy_params += self.alpha * grad\n",
        "        return entropy\n",
        "\n",
        "    def update_multiple_sampling(self, trajectories):\n",
        "        # Update policy parameters using the approximated gradient of the entropy objective function\n",
        "        grad = np.zeros_like(self.policy_params)\n",
        "        entropies = []\n",
        "        for episode, d_t in trajectories:\n",
        "            # Initialize the gradient of the k-th sampled trajectory\n",
        "            grad_k = np.zeros_like(self.policy_params)\n",
        "            # Compute entropy of d_t\n",
        "            entropy = self.compute_entropy(d_t)\n",
        "            # Compute the gradient\n",
        "            for t in range(len(episode)):\n",
        "                state, action, probs, _, _ = episode[t]\n",
        "                # Compute the policy gradient\n",
        "                dlogp = np.zeros(self.env.action_space.n)\n",
        "                for i in range(env.action_space.n):\n",
        "                    dlogp[i] = 1.0 - probs[i] if i == action else -probs[i]\n",
        "                grad_k += np.outer(state, dlogp) * entropy\n",
        "            # Sum the k-th gradient to the final gradient\n",
        "            grad += grad_k\n",
        "            entropies.append(entropy)\n",
        "        # Divide the gradient by the number of trajectories sampled\n",
        "        grad /= len(trajectories)\n",
        "        # Update the policy parameters\n",
        "        self.policy_params += self.alpha * grad\n",
        "        return entropies"
      ],
      "metadata": {
        "id": "kbIPRkwkEcNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gridworld POMDP\n"
      ],
      "metadata": {
        "id": "w74Pvlk5EidG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldPOMDPEnv(GridworldEnv):\n",
        "    '''\n",
        "    This class implements the extension of the upper Gridworld MDP, making it become a POMDP by inserting the observation mechanism.\n",
        "    The observations here are modeled as a probability of returning the real state of the MDP, in a Gaussian fashion based on the distance of the observed state from the real current state.\n",
        "\n",
        "    Args:\n",
        "     - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
        "     - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
        "     - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
        "     - steepness: a parameter to control the steepness of the Gaussian distribution that models the observation probability from the real state. A higher value makes it steeper.\n",
        "    '''\n",
        "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, randomize=0, steepness=15):\n",
        "        # Initialize the underlying Gridworld MDP\n",
        "        super().__init__(grid_size=grid_size, time_horizon=time_horizon, randomize=randomize, prob=prob)\n",
        "        # Initialize all the POMDP specific variables\n",
        "        self.observation_space = spaces.Discrete(self.state_space.n)\n",
        "        self.steepness = steepness\n",
        "        self.observation_matrix = self.build_observation_matrix()\n",
        "\n",
        "    def build_observation_matrix(self):\n",
        "        '''\n",
        "        This method creates the observation matrix for our environment.\n",
        "        The observation function indexes are to be used in order:\n",
        "         - first param: true state s\n",
        "         - second param: observation o\n",
        "        '''\n",
        "        # Initialize the observation function with zeros\n",
        "        observation_matrix = np.zeros((self.state_space.n, self.state_space.n))\n",
        "        # Calculate the variance of the Gaussian distribution based on the grid size\n",
        "        variance = (self.grid_size + self.length_corridor // 2) ** 2\n",
        "\n",
        "        for s_i in range(self.observation_space.n):\n",
        "            for s in range(self.observation_space.n):\n",
        "                # Calculate the distance between the observed position and the true state\n",
        "                distance = np.linalg.norm(np.array(self.index_to_state(s_i)) - np.array(self.index_to_state(s)))\n",
        "                # Assign probability based on Gaussian distribution with adjusted steepness\n",
        "                observation_matrix[s_i][s] += np.exp(-self.steepness * distance**2 / (2 * variance))\n",
        "            # Normalize the probabilities for each row\n",
        "            observation_matrix[s_i] /= np.sum(observation_matrix[s_i])\n",
        "\n",
        "        # Return the built matrix\n",
        "        return observation_matrix\n",
        "\n",
        "    def reset(self):\n",
        "        # Call the upper reset of the environment\n",
        "        super().reset()\n",
        "        return self.state_to_index(self.current_pos)\n",
        "        '''\n",
        "        # Set the initial belief and give it to the agent\n",
        "        initial_belief = self.observation_matrix[self.state_to_index(self.current_pos)]\n",
        "        return initial_belief\n",
        "        '''\n",
        "\n",
        "    def step(self, action):\n",
        "        # Save the true state\n",
        "        true_state = self.current_pos\n",
        "        # Make the step of the underlying MDP\n",
        "        next_state, reward, done, info = super().step(action)\n",
        "        # Get the index of the state\n",
        "        next_state_index = self.state_to_index(next_state)\n",
        "        # Get the observation probabilities for the state\n",
        "        obs_prob = self.observation_matrix[next_state_index]\n",
        "        # Sample the next observation from the probabilities\n",
        "        obs = np.random.choice(self.observation_space.n, p=obs_prob)\n",
        "        return obs, reward, done, False, true_state"
      ],
      "metadata": {
        "id": "YqI_ea2-EjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GridworldPOMDPEnvGoalless(GridworldEnvGoalless):\n",
        "    '''\n",
        "    This extension of the environment work functionally like the previous, but it removes the ending goal state.\n",
        "    In the POMDP case it also adds the observation mechanism.\n",
        "    '''\n",
        "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, steepness=15):\n",
        "        # Initialize the underlying Gridworld MDP\n",
        "        super().__init__(grid_size=grid_size, time_horizon=time_horizon, prob=prob)\n",
        "        # Initialize all the POMDP specific variables\n",
        "        self.observation_space = spaces.Discrete(self.state_space.n)\n",
        "        self.steepness = steepness\n",
        "        self.observation_matrix = self.build_observation_matrix()\n",
        "\n",
        "    def build_observation_matrix(self):\n",
        "        '''\n",
        "        This method creates the observation matrix for our environment.\n",
        "        The observation function indexes are to be used in order:\n",
        "         - first param: true state s\n",
        "         - second param: observation o\n",
        "        '''\n",
        "        # Initialize the observation function with zeros\n",
        "        observation_matrix = np.zeros((self.observation_space.n, self.observation_space.n))\n",
        "        # Calculate the variance of the Gaussian distribution based on the grid size\n",
        "        variance = (self.grid_size // 2) ** 2\n",
        "\n",
        "        for s_i in range(self.observation_space.n):\n",
        "            for s in range(self.observation_space.n):\n",
        "                # Calculate the distance between the observed position and the true state\n",
        "                distance = np.linalg.norm(np.array(self.index_to_state(s_i)) - np.array(self.index_to_state(s)))\n",
        "                # Assign probability based on Gaussian distribution with adjusted steepness\n",
        "                observation_matrix[s_i][s] += np.exp(-self.steepness * distance**2 / (2 * variance))\n",
        "            # Normalize the probabilities for each row\n",
        "            observation_matrix[s_i] /= np.sum(observation_matrix[s_i])\n",
        "\n",
        "        # Return the built matrix\n",
        "        return observation_matrix\n",
        "\n",
        "    def reset(self):\n",
        "        # Call the upper reset of the environment\n",
        "        super().reset()\n",
        "        return self.state_to_index(self.current_pos)\n",
        "        '''\n",
        "        # Set the initial belief and give it to the agent\n",
        "        initial_belief = self.observation_matrix[self.state_to_index(self.current_pos)]\n",
        "        return initial_belief\n",
        "        '''\n",
        "\n",
        "    def step(self, action):\n",
        "        # Save the true state\n",
        "        true_state = self.current_pos\n",
        "        # Make the step of the underlying MDP\n",
        "        next_state, reward, done, info = super().step(action)\n",
        "        # Get the index of the state\n",
        "        next_state_index = self.state_to_index(next_state)\n",
        "        # Get the observation probabilities for the state\n",
        "        obs_prob = self.observation_matrix[next_state_index]\n",
        "        # Sample the next observation from the probabilities\n",
        "        obs = np.random.choice(self.observation_space.n, p=obs_prob)\n",
        "        return obs, reward, done, False, true_state"
      ],
      "metadata": {
        "id": "_TezduPWEpu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gridworld POMDP BiModal"
      ],
      "metadata": {
        "id": "k2mFDnWUEvlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldPOMDPEnvBiModal(GridworldEnvGoalless):\n",
        "    '''\n",
        "    This class implements the extension of the upper Gridworld MDP, making it become a POMDP by inserting the observation mechanism.\n",
        "    The observations here are modeled as a probability of returning the real state of the MDP, in a Gaussian fashion based on the distance of the observed state from the real current state.\n",
        "\n",
        "    Args:\n",
        "     - grid_size: the size in height and length of the grid, N of the NxN matrix.\n",
        "     - time_horizon: the maximum number of time steps the agent can take to get to the goal. If set to -1 the time horizon is ∞.\n",
        "     - prob: the probability with which the environment takes the chosen action. If set to 0 the actions taken by the agent are deterministic.\n",
        "     - steepness: a parameter to control the steepness of the Gaussian distribution that models the observation probability from the real state. A higher value makes it steeper.\n",
        "    '''\n",
        "    def __init__(self, grid_size=5, time_horizon=-1, prob=0.1, randomize=0, steepness=15):\n",
        "        # Initialize the underlying Gridworld MDP\n",
        "        super().__init__(grid_size=grid_size, time_horizon=time_horizon, randomize=randomize, prob=prob)\n",
        "        # Initialize all the POMDP specific variables\n",
        "        self.shift_amount = 0.80\n",
        "        self.observation_space = spaces.Discrete(self.state_space.n)\n",
        "        self.steepness = steepness\n",
        "        self.observation_matrix = self.build_observation_matrix()\n",
        "\n",
        "    def build_observation_matrix(self):\n",
        "        '''\n",
        "        This method creates the observation matrix for our environment.\n",
        "        The observation function indexes are to be used in order:\n",
        "         - first param: true state s\n",
        "         - second param: observation o\n",
        "        '''\n",
        "        # Initialize the observation function with zeros\n",
        "        observation_matrix = np.zeros((self.state_space.n, self.state_space.n))\n",
        "        # Calculate the variance of the Gaussian distribution based on the grid size\n",
        "        variance = (self.grid_size // 2) ** 2\n",
        "\n",
        "        for s_i in range(self.observation_space.n):\n",
        "            for s in range(self.observation_space.n):\n",
        "                # Calculate the distance between the observed position and the true state\n",
        "                distance = np.linalg.norm(np.array(self.index_to_state(s_i)) - np.array(self.index_to_state(s)))\n",
        "                # Calculate the probability for the left-shifted Gaussian\n",
        "                prob_left = np.exp(-self.steepness * (distance + self.shift_amount) ** 2 / (2 * variance))\n",
        "                # Calculate the probability for the right-shifted Gaussian\n",
        "                prob_right = np.exp(-self.steepness * (distance - self.shift_amount) ** 2 / (2 * variance))\n",
        "                # Assign probability as the sum of left-shifted and right-shifted Gaussians\n",
        "                observation_matrix[s_i][s] += prob_left + prob_right\n",
        "\n",
        "            # Normalize the probabilities for each row\n",
        "            observation_matrix[s_i] /= np.sum(observation_matrix[s_i])\n",
        "\n",
        "        # Return the built matrix\n",
        "        return observation_matrix\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Call the upper reset of the environment\n",
        "        super().reset()\n",
        "        return self.state_to_index(self.current_pos)\n",
        "        '''\n",
        "        # Set the initial belief and give it to the agent\n",
        "        initial_belief = self.observation_matrix[self.state_to_index(self.current_pos)]\n",
        "        return initial_belief\n",
        "        '''\n",
        "\n",
        "    def step(self, action):\n",
        "        # Save the true state\n",
        "        true_state = self.current_pos\n",
        "        # Make the step of the underlying MDP\n",
        "        next_state, reward, done, info = super().step(action)\n",
        "        # Get the index of the state\n",
        "        next_state_index = self.state_to_index(next_state)\n",
        "        # Get the observation probabilities for the state\n",
        "        obs_prob = self.observation_matrix[next_state_index]\n",
        "        # Sample the next observation from the probabilities\n",
        "        obs = np.random.choice(self.observation_space.n, p=obs_prob)\n",
        "        return obs, reward, done, False, true_state"
      ],
      "metadata": {
        "id": "HyH2A10tEvYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gym.envs.register(\n",
        "    id='GridworldPOMDPEnvBiModal-v0',\n",
        "    entry_point=GridworldPOMDPEnvBiModal\n",
        ")"
      ],
      "metadata": {
        "id": "a2UfKJlnE53W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PtP7XxQeFrEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_trajectory(agent, env):\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode.append((state, action, reward, next_state))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    return episode, total_reward\n",
        "\n"
      ],
      "metadata": {
        "id": "EnPPJbZ8Fqgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMRL"
      ],
      "metadata": {
        "id": "wcGwdts1JKsq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qm52bbrxJLJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Trial\n"
      ],
      "metadata": {
        "id": "d1AWthKZFCA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCEAgentEPOMDP(REINFORCEAgentE):\n",
        "    '''\n",
        "    This calls implements the Reinforce Agent for the POMDP defined above.\n",
        "    It is implemented as the extension of the former MDP agent but adds the belief state and what is concerned by it.\n",
        "    '''\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.9, n_traj=50):\n",
        "        super().__init__(env=env, alpha=alpha, gamma=gamma)\n",
        "        self.beliefs = np.ones((n_traj, env.observation_space.n)) / env.observation_space.n\n",
        "        self.n_expected_value = 30\n",
        "\n",
        "    def get_actions(self):\n",
        "        actions = []\n",
        "        probs = []\n",
        "        for state in self.beliefs:\n",
        "            action, prob = self.get_action(state)\n",
        "            actions.append(action)\n",
        "            probs.append(prob)\n",
        "        return actions, probs\n",
        "\n",
        "    def belief_update(self, actions, observations):\n",
        "        batch_size = len(actions)\n",
        "\n",
        "        # Get the indices of the observed states\n",
        "        obs_state_indices = np.array([self.env.state_to_index(obs) for obs in observations])\n",
        "\n",
        "        # Get the observation probabilities for all states\n",
        "        obs_probabilities = self.env.observation_matrix[:, obs_state_indices]\n",
        "\n",
        "        # Calculate the numerator: Obs(o|s) * sum_s' (P(s|s',a) * belief_state[s'])\n",
        "        numerators = obs_probabilities * np.sum(self.env.transition_matrix[:, :, actions] * self.belief_state, axis=1)[:, np.newaxis]\n",
        "\n",
        "        # Calculate the denominator: sum_s' (Obs(o|s') * sum_s'' (P(s'|s'',a) * belief_state[s'']))\n",
        "        denominators = np.sum(obs_probabilities * np.sum(self.env.transition_matrix[:, :, actions] * self.belief_state, axis=1), axis=1)\n",
        "\n",
        "        # Update the belief states for each trajectory in the batch\n",
        "        for i in range(batch_size):\n",
        "            self.belief_state[i] = numerators[i] / denominators[i]\n",
        "\n",
        "    def get_states(self):\n",
        "        '''\n",
        "        This method is used to sample or compute the expected state given the current beliefs.\n",
        "        Args:\n",
        "        - behaviour: if 0, it returns the state from a single sampling.\n",
        "                     if 1, it returns the state from multiple samplings.\n",
        "                     No other values allowed.\n",
        "        '''\n",
        "        states = []\n",
        "        for state in self.beliefs:\n",
        "            states.append(self.get_state(state, 1))\n",
        "        return states\n",
        "\n",
        "    def get_state(self, belief, behaviour):\n",
        "        '''\n",
        "        This method is used to sample or compute the expected state given the current belief.\n",
        "        Args:\n",
        "         - behaviour: if 0 it returns the state from a single sampling.\n",
        "                 if 1 it returns the state from multiple samplings.\n",
        "                 No other values allowed.\n",
        "        '''\n",
        "        if behaviour != 0 and behaviour != 1:\n",
        "            raise Exception(\"You have to pass me 0 or 1, read :/\")\n",
        "\n",
        "        if behaviour == 0:\n",
        "            state = np.random.choice(belief.size, p=belief)\n",
        "            state = env.index_to_state(state)\n",
        "            return state\n",
        "\n",
        "        if behaviour == 1:\n",
        "            states = np.random.choice(belief.size, self.n_expected_value, p=belief)\n",
        "            state = mode(states).mode[0]\n",
        "            state = env.index_to_state(state)\n",
        "            return state"
      ],
      "metadata": {
        "id": "8sBQEOyqE-xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.utils.step_api_compatibility import step_to_new_api\n",
        "# Number of episodes for the training\n",
        "n_episodes = 8000\n",
        "n_traj = 50\n",
        "# Number of runs per episode\n",
        "n_run = 4\n",
        "# Define the arguments for each environment\n",
        "time_horizon = 30\n",
        "steepness = 15\n",
        "prob = 0\n",
        "envs = gym.vector.make('GridworldPOMDPEnvBiModal-v0', time_horizon=time_horizon, steepness=steepness, prob=prob, num_envs=n_traj, new_step_api=True)\n",
        "env = GridworldPOMDPEnvBiModal(time_horizon = time_horizon, steepness=15, prob=0)\n",
        "\n",
        "with tqdm(total=n_run * n_episodes, ncols=80) as pbar:\n",
        "    # Train the agent and plot the entropies\n",
        "    list_entropies = []\n",
        "    list_true_entropies = []\n",
        "    for r in range(n_run):\n",
        "        agent = REINFORCEAgentEPOMDP(env, alpha=0.5, n_traj=n_traj)  # Modify the agent to accept the parallel environments\n",
        "        avg_entropies = []\n",
        "        avg_true_entropies = []\n",
        "        for i in range(n_episodes):\n",
        "            episode = []\n",
        "            d_t = np.zeros(env.observation_space.n)  # Initialize visitation counts for all parallel environments\n",
        "            true_d_t = np.zeros(env.observation_space.n)\n",
        "            envs.reset()\n",
        "            done = np.zeros(n_traj, dtype=bool)\n",
        "            while not np.all(done):\n",
        "                #print(agent.beliefs)\n",
        "                # Sample action and get probabilities from the belief\n",
        "                actions, probs = agent.get_actions()\n",
        "                # Sample state\n",
        "                sampled_states = agent.get_states()\n",
        "                # Take a step in the parallel environments\n",
        "                print(actions)\n",
        "                next_obs, rewards, done, _, true_states = envs.step(actions)\n",
        "                # Get the indices of the states for all parallel environments\n",
        "                state_indices = [env.state_to_index(state) for state in sampled_states]\n",
        "                true_state_indices = [env.state_to_index(state) for state in true_states]\n",
        "                # Update state visitation counts for all parallel environments\n",
        "                for i, state_index in enumerate(state_indices):\n",
        "                    d_t[state_index] += 1\n",
        "                for i, true_state_index in enumerate(true_state_indices):\n",
        "                    true_d_t[true_state_index] += 1\n",
        "                episode.append((agent.belief_states, actions, probs, rewards, true_states))\n",
        "                agent.belief_update(actions, next_obs)\n",
        "            # Compute true entropy of the trajectory for all parallel environments\n",
        "            true_d_t /= len(episode)\n",
        "            true_entropies.append(agent.compute_entropy(true_d_t))\n",
        "            d_t /= len(episode)\n",
        "            episodes.append((episode, d_t))\n",
        "            entropies = agent.update_multiple_sampling(episodes)\n",
        "            avg_entropies.append(np.mean(entropies))\n",
        "            avg_true_entropies.append(np.mean(true_entropies))\n",
        "            pbar.update(1)\n",
        "        agent.print_visuals(envs=envs, n_traj=n_traj)  # Modify the agent to accept the parallel environments\n",
        "        list_entropies.append(avg_entropies)\n",
        "        list_true_entropies.append(avg_true_entropies)\n",
        "    list_entropies = np.transpose(np.array(list_entropies), (1, 0))\n",
        "    list_true_entropies = np.transpose(np.array(list_true_entropies), (1, 0))\n",
        "\n",
        "plot_graph(n_run, n_episodes, list_entropies, list_true_entropies, confidence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "id": "kRG59IkeFC1W",
        "outputId": "220a45a9-0179-446a-d27a-2134342c9f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                                 | 0/32000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "<ipython-input-11-696a85eaaada>:70: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
            "  state = mode(states).mode[0]\n",
            "  0%|                                                 | 0/32000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 3, 1, 2, 3, 2, 1, 1, 2, 3, 3, 3, 0, 3, 3, 1, 1, 1, 0, 2, 0, 3, 2, 1, 0, 2, 1, 0, 0, 2, 3, 3, 1, 1, 3, 2, 3, 2, 1, 2, 0, 1, 2, 3, 2, 0, 1, 3, 3]\n",
            "<multiprocessing.connection.Connection object at 0x785e792b37f0>\n",
            "False\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aca6f6bcbd63>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Take a step in the parallel environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Get the indices of the states for all parallel environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mstate_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampled_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/vector/vector_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/vector/async_vector_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_api_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0msuccesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep_api_compatibility\u001b[0;34m(step_returns, new_step_api, is_vector_env)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vector_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_old_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vector_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep_to_new_api\u001b[0;34m(step_returns, is_vector_env)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mis_vector_env\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstep_returns\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvector\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}